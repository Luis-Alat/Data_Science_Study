{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the content in the current notebook is based on the explanations found on [sparkbyexamples](https://sparkbyexamples.com/pyspark/pyspark-rdd-transformations/#transformation-types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import trim, length, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/03 23:01:42 WARN Utils: Your hostname, IdeaPad-L340 resolves to a loopback address: 127.0.1.1; using 192.168.100.16 instead (on interface wlp1s0)\n",
      "23/09/03 23:01:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/03 23:01:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.16:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>transformations_actions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f25ec3d2980>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.\n",
    "    builder.\n",
    "    appName(\"transformations_actions\").\n",
    "    master(\"local[4]\").\n",
    "    getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " time         | 631153353990         \n",
      " place        | 12 km NNW of Mead... \n",
      " status       | reviewed             \n",
      " tsunami      | 0                    \n",
      " significance | 96                   \n",
      " data_type    | earthquake           \n",
      " magnitudo    | 2.5                  \n",
      " state        |  Alaska              \n",
      " longitude    | -149.6692            \n",
      " latitude     | 61.7302              \n",
      " depth        | 30.1                 \n",
      " date         | 1989-12-31 18:22:... \n",
      "-RECORD 1----------------------------\n",
      " time         | 631153491210         \n",
      " place        | 14 km S of Volcan... \n",
      " status       | reviewed             \n",
      " tsunami      | 0                    \n",
      " significance | 31                   \n",
      " data_type    | earthquake           \n",
      " magnitudo    | 1.41                 \n",
      " state        |  Hawaii              \n",
      " longitude    | -155.2123333         \n",
      " latitude     | 19.3176667           \n",
      " depth        | 6.585                \n",
      " date         | 1989-12-31 18:24:... \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = \"data/earthquakes/Eartquakes-1990-2023.csv\"\n",
    "earthquakes_ps = spark.read.csv(file, sep=\",\",\n",
    "                               inferSchema=True, header=True)\n",
    "\n",
    "earthquakes_ps.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations are lazy operations meaning none of the transformations get executed until you call an action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Narrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Narrow transformations are the result of map() and filter() functions and these compute data that live on a single partition meaning there will not be any data movement between partitions to execute narrow transformations_ (no shuffling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Wider transformations are the result of groupByKey() and reduceByKey() functions and these compute data that live on many partitions meaning there will be data movements between partitions to execute wider transformations_ (shuffling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Wide and Narrow transformation functions are:\n",
    "\n",
    "* __cache()__:\tCaches the RDD\n",
    "\n",
    "* __filter()__:\tReturns a new RDD after applying filter function on source dataset.\n",
    "\n",
    "* __flatMap()__:\tReturns flattern map meaning if you have a dataset with array, it converts each elements in a array as a row. In other words it return 0 or more items in output for each element in dataset. (Mapping one-to-many)\n",
    "\n",
    "* __map()__:\tApplies transformation function on dataset and returns same number of elements in distributed dataset. (Mapping one-to-one to each element of the row)\n",
    "\n",
    "* __mapPartitions()__:\tSimilar to map, but executs transformation function on each partition, This gives better performance than map function.\n",
    "\n",
    "* __mapPartitionsWithIndex()__:\tSimilar to map Partitions, but also provides func with an integer value representing the index of the partition.\n",
    "\n",
    "* __randomSplit()__:\tSplits the RDD by the weights specified in the argument. For example * rdd.randomSplit(0.7,0.3)\n",
    "\n",
    "* __union()__:\tComines elements from source dataset and the argument and returns combined dataset. This is similar to union function in Math set operations.\n",
    "\n",
    "* __sample()__:\tReturns the sample dataset.\n",
    "\n",
    "* __intersection()__: Returns the dataset which contains elements in both source dataset and an argument (similar to intesect in SQL)\n",
    "\n",
    "* __distinct()__: Returns the dataset by eliminating all duplicated elements.\n",
    "\n",
    "* __repartition()__: Return a dataset with number of partition specified in the argument. This operation reshuffles the RDD randamly, It could either return lesser or more partioned RDD based on the input supplied. __repartition__ is an expensive operation since involves shuffling.\n",
    "\n",
    "* __coalesce()__: Similar to repartition by operates better when we want to the decrease the partitions. Betterment acheives by reshuffling the data from fewer nodes compared with all nodes by repartition. It's less expensive than repartition since minimizes data movement and shuffling.\n",
    "\n",
    "* __join()__: Merge two tables based on a common column ID\n",
    "\n",
    "* __groupBy()__: Group By operation similiar to groupby pandas dataframe or GROUP BY in SQL\n",
    "\n",
    "* __agg()__: Run some function over a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes_ps = earthquakes_ps.withColumn(\"state\", trim(\"state\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " time         | 631564020120         \n",
      " place        | 189 km SW of La C... \n",
      " status       | reviewed             \n",
      " tsunami      | 0                    \n",
      " significance | 554                  \n",
      " data_type    | earthquake           \n",
      " magnitudo    | 6.0                  \n",
      " state        | Mexico               \n",
      " longitude    | -106.795             \n",
      " latitude     | 18.863               \n",
      " depth        | 33.0                 \n",
      " date         | 1990-01-05 12:27:... \n",
      "-RECORD 1----------------------------\n",
      " time         | 632196446840         \n",
      " place        | 5 km N of El Cort... \n",
      " status       | reviewed             \n",
      " tsunami      | 0                    \n",
      " significance | 432                  \n",
      " data_type    | earthquake           \n",
      " magnitudo    | 5.3                  \n",
      " state        | Mexico               \n",
      " longitude    | -99.509              \n",
      " latitude     | 16.826               \n",
      " depth        | 28.4                 \n",
      " date         | 1990-01-12 20:07:... \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter function with trimmed column\n",
    "earthquakes_ps.filter(earthquakes_ps[\"state\"] == \"Mexico\").show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|               state|length|\n",
      "+--------------------+------+\n",
      "|Montenegro-Albani...|    32|\n",
      "|Missouri-Arkansas...|    31|\n",
      "|Missouri-Illinois...|    31|\n",
      "|Macedonia-Serbia ...|    30|\n",
      "|Mexico-Guatemala ...|    30|\n",
      "|Myanmar-Thailand ...|    30|\n",
      "|Mongolia-China bo...|    28|\n",
      "|Myanmar-China bor...|    27|\n",
      "|Myanmar-India bor...|    27|\n",
      "|Mauritius - Reuni...|    26|\n",
      "|Macquarie Island ...|    23|\n",
      "|Marshall Islands ...|    23|\n",
      "|Maldive Islands r...|    22|\n",
      "|Mariana Islands r...|    22|\n",
      "|Midway Islands re...|    21|\n",
      "|  Mozambique Channel|    18|\n",
      "|    Macedonia region|    16|\n",
      "|    Marshall Islands|    16|\n",
      "|    Mid-Indian Ridge|    16|\n",
      "|      Mayotte region|    14|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter with regex and orderBy\n",
    "(\n",
    "    earthquakes_ps.\n",
    "    filter(col(\"state\").rlike(r\"^\\s*Mm?.+\")).\n",
    "    select(\"state\").\n",
    "    distinct().\n",
    "    withColumn(\"length\", length(col(\"state\"))).\n",
    "    orderBy(col(\"length\").desc(), col(\"state\").asc()).\n",
    "    show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+\n",
      "| id|               state|one|\n",
      "+---+--------------------+---+\n",
      "|  1|Alaska my_new_string|  1|\n",
      "|  2|Hawaii my_new_string|  1|\n",
      "|  3|California my_new...|  1|\n",
      "|  4|California my_new...|  1|\n",
      "|  5|California my_new...|  1|\n",
      "|  6|California my_new...|  1|\n",
      "|  7|California my_new...|  1|\n",
      "|  8|California my_new...|  1|\n",
      "|  9|California my_new...|  1|\n",
      "| 10|Washington my_new...|  1|\n",
      "+---+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FlatMap function\n",
    "counter = 0\n",
    "def add_string_state_column(row):\n",
    "    global counter\n",
    "    \n",
    "    state = row.state + \" \" + \"my_new_string\"\n",
    "    counter += 1\n",
    "    \n",
    "    return [(counter, state, 1)]\n",
    "\n",
    "my_new_string_results = (\n",
    "    earthquakes_ps.\n",
    "    select(\"data_type\", \"state\").\n",
    "    rdd.\n",
    "    flatMap(add_string_state_column)\n",
    ")\n",
    "\n",
    "my_new_string_results_df = spark.createDataFrame(my_new_string_results,\n",
    "                                                 schema=[\"id\", \"state\", \"one\"])\n",
    "my_new_string_results_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('earthquake_type', 'Alaska_state', 2.5),\n",
       " ('earthquake_type', 'Hawaii_state', 1.41),\n",
       " ('earthquake_type', 'California_state', 1.11),\n",
       " ('earthquake_type', 'California_state', 0.98),\n",
       " ('earthquake_type', 'California_state', 2.95),\n",
       " ('earthquake_type', 'California_state', 2.77),\n",
       " ('earthquake_type', 'California_state', 1.13),\n",
       " ('earthquake_type', 'California_state', 0.83),\n",
       " ('earthquake_type', 'California_state', 1.59),\n",
       " ('earthquake_type', 'Washington_state', 2.2)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map function\n",
    "my_new_string_results = (\n",
    "    earthquakes_ps.\n",
    "    select(\"data_type\", \"state\", \"magnitudo\").\n",
    "    rdd.\n",
    "    map(lambda x: (str(x[0]) + \"_type\", str(x[1]) + \"_state\", float(x[2])))\n",
    ")\n",
    "\n",
    "my_new_string_results.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------\n",
      " time         | 833770364950                          \n",
      " place        | 21 km ENE of San Lucas, California    \n",
      " status       | reviewed                              \n",
      " tsunami      | 0                                     \n",
      " significance | 31                                    \n",
      " data_type    | earthquake                            \n",
      " magnitudo    | 1.41                                  \n",
      " state        | California                            \n",
      " longitude    | -120.8031667                          \n",
      " latitude     | 36.2193333                            \n",
      " depth        | 6.246                                 \n",
      " date         | 1996-06-02 21:52:44.95                \n",
      "-RECORD 1---------------------------------------------\n",
      " time         | 871388234520                          \n",
      " place        | 7 km NNE of West Yellowstone, Montana \n",
      " status       | reviewed                              \n",
      " tsunami      | 0                                     \n",
      " significance | 14                                    \n",
      " data_type    | earthquake                            \n",
      " magnitudo    | 0.97                                  \n",
      " state        | Montana                               \n",
      " longitude    | -111.0725                             \n",
      " latitude     | 44.7223333                            \n",
      " depth        | 11.7                                  \n",
      " date         | 1997-08-12 07:17:14.52                \n",
      "-RECORD 2---------------------------------------------\n",
      " time         | 1098821412857                         \n",
      " place        | 5 km NNW of Clam Gulch, Alaska        \n",
      " status       | reviewed                              \n",
      " tsunami      | 0                                     \n",
      " significance | 39                                    \n",
      " data_type    | earthquake                            \n",
      " magnitudo    | 1.6                                   \n",
      " state        | Alaska                                \n",
      " longitude    | -151.4188                             \n",
      " latitude     | 60.2818                               \n",
      " depth        | 49.9                                  \n",
      " date         | 2004-10-26 15:10:12.857               \n",
      "-RECORD 3---------------------------------------------\n",
      " time         | 1307281936189                         \n",
      " place        | 12 km SSE of Trapper Creek, Alaska    \n",
      " status       | reviewed                              \n",
      " tsunami      | 0                                     \n",
      " significance | 12                                    \n",
      " data_type    | earthquake                            \n",
      " magnitudo    | 0.9                                   \n",
      " state        | Alaska                                \n",
      " longitude    | -150.1836                             \n",
      " latitude     | 62.2105                               \n",
      " depth        | 9.2                                   \n",
      " date         | 2011-06-05 08:52:16.189               \n",
      "-RECORD 4---------------------------------------------\n",
      " time         | 1344162619630                         \n",
      " place        | 16 km SSW of Levan, Utah              \n",
      " status       | reviewed                              \n",
      " tsunami      | 0                                     \n",
      " significance | 12                                    \n",
      " data_type    | earthquake                            \n",
      " magnitudo    | 0.88                                  \n",
      " state        | Utah                                  \n",
      " longitude    | -111.9098333                          \n",
      " latitude     | 39.4155                               \n",
      " depth        | 9.52                                  \n",
      " date         | 2012-08-05 05:30:19.63                \n",
      "-RECORD 5---------------------------------------------\n",
      " time         | 1447165185140                         \n",
      " place        | south of the Kermadec Islands         \n",
      " status       | reviewed                              \n",
      " tsunami      | 0                                     \n",
      " significance | 369                                   \n",
      " data_type    | earthquake                            \n",
      " magnitudo    | 4.9                                   \n",
      " state        | south of the Kermadec Islands         \n",
      " longitude    | -178.4042                             \n",
      " latitude     | -32.8126                              \n",
      " depth        | 10.0                                  \n",
      " date         | 2015-11-10 08:19:45.14                \n",
      "-RECORD 6---------------------------------------------\n",
      " time         | 1674812821451                         \n",
      " place        | 36 km W of Anchor Point, Alaska       \n",
      " status       | reviewed                              \n",
      " tsunami      | 0                                     \n",
      " significance | 35                                    \n",
      " data_type    | earthquake                            \n",
      " magnitudo    | 1.5                                   \n",
      " state        | Alaska                                \n",
      " longitude    | -152.4892                             \n",
      " latitude     | 59.7907                               \n",
      " depth        | 79.2                                  \n",
      " date         | 2023-01-27 03:47:01.451               \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Taking a sample of the 0.000001 percent of the total rows\n",
    "sample_examples = earthquakes_ps.sample(fraction=0.000001)\n",
    "sample_size = sample_examples.count()\n",
    "\n",
    "print(f\"Sample size: {sample_size}\\n\")\n",
    "sample_examples.show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9941378714241778, 1.8647143033827378, 1.618168308839519, 1.6156452461095947]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MapPartitions\n",
    "def avg_magnitude_earthquakes_by_partition(partition_iterator):\n",
    "\n",
    "    sum = 0\n",
    "    size_partition = 0\n",
    "\n",
    "    # Iterating over the current partition\n",
    "    for row in partition_iterator:\n",
    "        sum += row.magnitudo\n",
    "        size_partition += 1\n",
    "\n",
    "    mean = sum / size_partition\n",
    "    return [mean]\n",
    "\n",
    "results_by_partition = (\n",
    "    earthquakes_ps.\n",
    "    select(\"state\", \"magnitudo\").\n",
    "    rdd.\n",
    "    mapPartitions(avg_magnitude_earthquakes_by_partition)\n",
    ")\n",
    "\n",
    "results_by_partition.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of partitions: 4\n",
      "\n",
      "Creating new partitions (repartition). This operation is expensive!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNew number of partitions: 8\n"
     ]
    }
   ],
   "source": [
    "# repartition\n",
    "print(f\"Current number of partitions: {earthquakes_ps.rdd.getNumPartitions()}\")\n",
    "\n",
    "print(f\"\\nCreating new partitions (repartition). This operation is expensive!\")\n",
    "earthquakes_ps = earthquakes_ps.repartition(8)\n",
    "print(f\"\\tNew number of partitions: {earthquakes_ps.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of partitions: 8\n",
      "\n",
      "Reducing the number of partitions (coalesce).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNew number of partitions: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:===========================================>              (3 + 1) / 4]\r"
     ]
    }
   ],
   "source": [
    "# coalesce\n",
    "print(f\"Current number of partitions: {earthquakes_ps.rdd.getNumPartitions()}\")\n",
    "\n",
    "print(f\"\\nReducing the number of partitions (coalesce).\")\n",
    "earthquakes_ps = earthquakes_ps.coalesce(4)\n",
    "print(f\"\\tNew number of partitions: {earthquakes_ps.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_RDD actions are PySpark operations that return the values to the driver program. Any function on RDD that returns other than RDD is considered as an action in PySpark programming_\n",
    "\n",
    "* __aggregate()__ over RDD: Aggregate the elements of each partition, and then the results for all the partitions, using a given combine functions and a neutral \"zero value.\" This function expects to functions, one for aggregate each partition and the other one to aggregate the results for all partitions.\n",
    "\n",
    "* aggregation functions over RDD (__count()__, __countByValue()__, __min()__, __max()__ ,etc)\n",
    "\n",
    "* display functions (__show()__, __take()__, __first()__, __head()__, __top()__, etc)\n",
    "\n",
    "* __collect()__: Run the respective tasks in the DAG and bring the results into the current machine.\n",
    "\n",
    "* __fold()__: Aggregate the elements of each partition, and then the results for all the partitions. Similar to __aggregate__ but this function only allows for one general function to aggregate instead of two.\n",
    "\n",
    "* __reduce()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note how seqOp returns two values and that's the reason why using combOp we need to access to the index 0 and 1\n",
    "seqOp = (lambda initial_values, individual_rdd_values: (initial_values[0] + individual_rdd_values,\n",
    "                                                        initial_values[1] + 1))\n",
    "\n",
    "# Note how comOp is supossed to return two values and thats the reason we obtained a tuple of len 2\n",
    "combOp = (lambda initial_values, results_by_partition: (initial_values[0] + results_by_partition[0],\n",
    "                                                        initial_values[1] + results_by_partition[1]))\n",
    "\n",
    "spark.sparkContext.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78748758.62224072"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An object of type Row is passed to seqOp\n",
    "seqOp  = lambda x, y: x + y.depth\n",
    "\n",
    "# An object of type float now is passed to combOP\n",
    "combOP = lambda x, y: x + y\n",
    "\n",
    "earthquakes_ps.select(\"depth\").rdd.aggregate(0, seqOp, combOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78748758.62224072"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this case, now an object of type float is passed to seqOp and an object of type float is passed to combOP\n",
    "seqOp  = lambda x, y: x + y\n",
    "combOP = lambda x, y: x + y\n",
    "\n",
    "flatten_func = lambda y: [y.depth]\n",
    "earthquakes_ps.select(\"depth\").rdd.flatMap(flatten_func).aggregate(0, seqOp, combOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
